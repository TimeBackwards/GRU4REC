{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fcd15890",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a5aa37d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(object):\n",
    "    def __init__(self, path, sep=',', session_key='SessionID', item_key='ItemID', time_key = 'Time', n_sample=10, itemmap = None, itemstamp = None, time_sort = False):\n",
    "        self.df = pd.read_csv(path, sep=sep, dtype = {session_key: int, item_key: int, time_key: float})\n",
    "        self.session_key = session_key\n",
    "        self.item_key = item_key\n",
    "        self.time_key = time_key\n",
    "        self.time_sort = time_sort\n",
    "        if n_sample > 0:\n",
    "            self.df = self.df[:n_sample]\n",
    "        \n",
    "        # Add column item index to data\n",
    "        self.add_item_indices(itemmap = itemmap)\n",
    "        '''\n",
    "        Sort the df by time, and the by session ID. That is, df is sorted by session ID and clicks within a session are next to each other, wherw the clicks within a session are time-ordered.\n",
    "        '''\n",
    "        \n",
    "        self.df.sort_values([session_key, time_key], inplace = True)\n",
    "        self.click_offsets = self.get_click_offset()\n",
    "        self.session_idx_arr = self.order_session_idx()\n",
    "        \n",
    "    def add_item_indices(self, itemmap=None):\n",
    "        '''\n",
    "        Add item index column named \"item_idx\" to the df\n",
    "        Args:\n",
    "        itemmap (pd.DataFrame): mapping between the item Ids and indices\n",
    "        '''\n",
    "        \n",
    "        if itemmap is None:\n",
    "            item_ids = self.df[self.item_key].unique()\n",
    "            item2idx = pd.Series(data = np.arange(len(item_ids)), index = item_ids)\n",
    "            itemmap = pd.DataFrame({self.item_key: item_ids, 'item_idx' : item2idx[item_ids].values})\n",
    "        \n",
    "        self.itemmap = itemmap\n",
    "        self.df = pd.merge(self.df, self.itemmap, on=self.item_key, how = 'inner')\n",
    "        \n",
    "    def get_click_offset(self):\n",
    "        '''\n",
    "        self.df[self.session_key] return a set of session_key\n",
    "        slef.df[self.session_key].nunique() return the size of session_key set (int)\n",
    "        self.df.groupby(self.session _key).size() return the size of each session_id\n",
    "        self.df.groupby(self.session_key).size().cumsum() return cumulative sum\n",
    "        '''\n",
    "        offsets = np.zeros(self.df[self.session_key].nunique() + 1, dtype = np.int32)\n",
    "        offsets[1:] = self.df.groupby(self.session_key).size().cumsum()\n",
    "        return offsets\n",
    "    \n",
    "    def order_session_idx(self):\n",
    "        if self.time_sort:\n",
    "            sessions_start_time = self.df.groupby(self.session_key)[self.time_key].min().values\n",
    "            session_idx_arr = np.argsort(sessions_start_time)\n",
    "        else:\n",
    "            session_idx_arr = np.arange(self.df[self.session_key].nunique())\n",
    "        return session_idx_arr\n",
    "        \n",
    "    @property\n",
    "    def items(self):\n",
    "        return self.itemmap[self.item_key].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "889d2335",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    def __init__(self, dataset, batch_size = 50):\n",
    "        '''\n",
    "        A class for creating session-parallel mini-batches.\n",
    "        \n",
    "        Args:\n",
    "            dataset (SessionDataset): the session dataset to generate the batches from\n",
    "            batch_size (int): size of the batch\n",
    "        '''\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def __iter__(self):\n",
    "        '''\n",
    "        Returns the iterator for producing session-parallel training mini-batches.\n",
    "        \n",
    "        Tields:\n",
    "            input (B,): torch.FloatTensor. Item indices tha will be endcoded as one-hot vectors later.\n",
    "            target (B,): a Variable tha stores the target item indices\n",
    "            masks: Numpy arrayindicating the positions of the sessions to be terminated\n",
    "        '''\n",
    "        \n",
    "        df = self.dataset.df\n",
    "        click_offsets = self.dataset.click_offsets\n",
    "        session_idx_arr = self.dataset.session_idx_arr\n",
    "        \n",
    "        iters = np.arange(self.batch_size)\n",
    "        maxiter = iters.max()\n",
    "        start = click_offsets[session_idx_arr[iters]]\n",
    "        end = click_offsets[session_idx_arr[iters] + 1]\n",
    "        mask = []\n",
    "        finished = False\n",
    "        \n",
    "        while not finished:\n",
    "            minlen = (end - start).min()\n",
    "            idx_target = df.item_idx.values[start]\n",
    "            \n",
    "            for i in range(minlen -1):\n",
    "                idx_input = idx_target\n",
    "                idx_target = df.item_idx.values[start + i + 1]\n",
    "                input = torch.LongTensor(idx_input)\n",
    "                target = torch.LongTensor(idx_target)\n",
    "                yield input, target, mask\n",
    "                \n",
    "            start = start + (minlen -1)\n",
    "            mask = np.arange(len(iters))[(end - start) <= 1]\n",
    "            for idx in mask:\n",
    "                maxiter += 1\n",
    "                if maxiter >= len(click_offsets) -1:\n",
    "                    finished = True\n",
    "                    break\n",
    "                    \n",
    "                iters[idx] = maxiter\n",
    "                start[idx] = click_offsets[session_idx_arr[maxiter]]\n",
    "                end[idx] = click_offsets[session_idx_arr[maxiter] + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c65ac156",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = 'recSys15TrainOnly.txt'\n",
    "valid_data_path = 'recSys15Valid.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a61c7a48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Recommendation/GRU4REC\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2d046ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = Dataset(os.path.join('../YooChoose/', train_data_path))\n",
    "valid_data = Dataset(os.path.join('../YooChoose/', valid_data_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cd6b2cb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37298"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data.items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f9236bb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7966041"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data.session_idx_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8678b5c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data.session_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8211bbcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.Dataset object at 0x7f8f1ae338b0>\n"
     ]
    }
   ],
   "source": [
    "print(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b6f091",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
